{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39167a6b-47ae-4a60-bb6f-8b4f0ac05173",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates how to generate images of handwritten digits using a `Deep Convolutional Generative Adversarial Network`.\n",
    "\n",
    "#### The code is written using the Keras Sequential API with a `tf.GradientTape`\n",
    "\n",
    "- Two models are trained simultaneously by an adversarial process. \n",
    "\n",
    "- A generator `artist` learns how to create images that look real while a discriminator `the art critic` learns how to tell real images from the fake ones.\n",
    "\n",
    "This notebook demonstratesthis process on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c735f6-7baf-40db-9424-082bfa868843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 19:02:14.967259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-11 19:02:14.967297: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PIL\n",
    "import time\n",
    "import glob\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8689990-323b-4ece-80a2-02080b3934d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 19:02:21.413890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-11 19:02:21.413937: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-11 19:02:21.413967: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (debonair): /proc/driver/nvidia/version does not exist\n",
      "2022-07-11 19:02:21.414327: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-11 19:02:21.415925: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and preprocess the dataset.\n",
    "BUFFER_SIZE = 60_000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 #Normalize images to [-1, 1]\n",
    "\n",
    "#Batch and shuffle the data.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da5542-3614-4f31-88bd-6263d6d90524",
   "metadata": {},
   "source": [
    "## 2. Create the models.\n",
    "\n",
    "#### Both the genrator and discriminator are defined using the `keras Sequential API.`\n",
    "\n",
    "#### a) The Generator.\n",
    "- The genrator uses the `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with the `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image of `28 * 28 * 1`. \n",
    "\n",
    "- Notice that the `tf.keras.layers.LeakyReLU` activation for each layer, except for the output layer which uses the `tanh` activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf81874-9f3f-4378-87b0-53ad3a09199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units = 7 * 7 * 256, use_bias = False, input_shape = (100, )))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) #None is the batch size.\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides = (1, 1), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    \n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d40e883-987a-4caf-bce6-76e0c6213f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use the untrained generator to create an image.\n",
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training = False)\n",
    "\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895d547-1644-4ddb-9c62-93b2ce2e3f4a",
   "metadata": {},
   "source": [
    "#### b) The Discriminator.\n",
    "The discriminator is a CNN-based image classefier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6430fd-3ee3-4771-83bd-e7582f151d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides = (2, 2), padding = 'same', input_shape = (28, 28, 1)))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides = (2, 2), padding = 'same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b95dfd4-5b62-4f61-8ef7-2c303e6a2649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00355594]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Use the untrained discriminator to classify the generated images as real or fake. \n",
    "#The model will be trained to output positive velues for real images and negative values for fake images.\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8419c-7dd9-4537-9e7e-234af3828b13",
   "metadata": {},
   "source": [
    "## 3. Define the loss and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56334e4a-d3a1-4e90-af61-a5f42ea1fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method returns a helper function to compute the cross-entropy loss.\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1318c044-725b-47ae-8826-5a57e623f215",
   "metadata": {},
   "source": [
    "#### Discriminator loss.\n",
    "- This method quantifies how well the discriminator is able to distinguish real images from fakes. \n",
    "\n",
    "- It compares the discriminator's predictions on real images to an array of 1s , and the discriminator predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "376fb574-0607-42ff-bb9f-ae3ea4f83f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eae553-490b-4b0f-8963-467aafdcb8ec",
   "metadata": {},
   "source": [
    "#### Generator loss.\n",
    "- The generator's loss quantifies how well it was able to trick the discriminator. \n",
    "- Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). \n",
    "- Here, we will compare the discriminator's decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0933b74-bed0-4df5-a559-88cf81ac3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e16352-cea9-48d6-ac8b-9d473bff1b70",
   "metadata": {},
   "source": [
    "- The discriminator and the generator optimizers are different since we will train 2 networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ddd0709-4f40-402f-88dc-8eb8b38ad5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c391-0baf-4474-9b52-c30f6fb64c47",
   "metadata": {},
   "source": [
    "#### Save checkpoints.\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case of a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "923a7c23-1011-43b3-b54c-867f4e514e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer, \n",
    "                                discriminator_optimizer = discriminator_optimizer, \n",
    "                                generator = generator, \n",
    "                                discriminator = discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ce63c-28ba-409d-8413-43941399f3fa",
   "metadata": {},
   "source": [
    "## 4. Define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "096e0211-8cbd-42e6-b3ac-2b8f2afb2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "#We'll reuse the seed over time to visualize progress.\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fc5c8-eb53-428d-956c-3eba65a8e423",
   "metadata": {},
   "source": [
    "- The training loop begins with the generator receiving a random seed as input. That seed is used to produce an image. \n",
    "- The discriminator is then used to classify real images (drawn from the train set) and fake images (produced by the generator)\n",
    "- The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8598897-8d31-4d1b-9366-afa476e28b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training = True)\n",
    "        real_output = discriminator(images, training = True)\n",
    "        fake_output = discriminator(generated_images, training = True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        \n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47da10e1-68b9-45b3-8b17-6a160b54be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "            \n",
    "    #Produce images for the GIF as we go.\n",
    "    display.clear_output(wait = True)\n",
    "    generate_and_save_images(generator, epoch + 1, seed)\n",
    "    \n",
    "    #Save model every 15 epochs.\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))\n",
    "    \n",
    "    #Generate after the final epoch.\n",
    "    display.clear_output(wait = True)\n",
    "    generate_and_save_images(generator, epochs, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa4283fc-b4c6-4649-8986-6ec9403e0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate and save images.\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    #Notice training is set to False so all the layers run in inference mode (batchnorm)\n",
    "    predictions = model(test_input, training = False)\n",
    "    \n",
    "    fig = plt.figure(figsize = (5, 5))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.savefig('image_at_epoch_{:.04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a162f1-6f4b-4b41-88b9-fa03c18e7b6f",
   "metadata": {},
   "source": [
    "# 5. Train the model.\n",
    "\n",
    "- Call the `train()` method defined above to train the generator and discriminator simultaneously. \n",
    "- At the beginning of the training, the generated images look loke random noise. As the training progresses, the generated digits will look increasingly real. \n",
    "- After about 50 epochs, they resemble MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "146c44e7-4575-469e-8f57-76c9153962bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_135518/357106148.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#Produce images for the GIF as we go.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde9e46-bedc-4867-8c87-aea8324c8f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
